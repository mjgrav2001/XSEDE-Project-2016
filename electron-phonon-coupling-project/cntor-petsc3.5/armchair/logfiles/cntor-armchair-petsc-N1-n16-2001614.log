[nid18916:24367] opal_os_dirpath_create: Error: Unable to create the sub-directory (/tmp/work) of (/tmp/work/n8d/openmpi-sessions-11578@nid18916_0/17174/1/0), mkdir failed [1]
[nid18916:24367] [[17174,1],0] ORTE_ERROR_LOG: Error in file util/session_dir.c at line 107
[nid18916:24367] [[17174,1],0] ORTE_ERROR_LOG: Error in file util/session_dir.c at line 403
[nid18916:24367] [[17174,1],0] ORTE_ERROR_LOG: Error in file base/ess_base_std_app.c at line 197
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_session_dir failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
[nid18916:24367] [[17174,1],0] ORTE_ERROR_LOG: Error in file ess_pmi_module.c at line 243
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_ess_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: orte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[nid18916:24367] *** An error occurred in MPI_Init_thread
[nid18916:24367] *** reported by process [1125515265,0]
[nid18916:24367] *** on a NULL communicator
[nid18916:24367] *** Unknown error
[nid18916:24367] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[nid18916:24367] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI process is aborting at a time when it cannot guarantee that all
of its peer processes in the job will be killed properly.  You should
double check that everything has shut down cleanly.

  Reason:     Before MPI_INIT completed
  Local host: nid18916
  PID:        24367
--------------------------------------------------------------------------
Application 5587734 exit codes: 1
Application 5587734 resources: utime ~1s, stime ~0s, Rss ~6680, inblocks ~23102, outblocks ~74434
